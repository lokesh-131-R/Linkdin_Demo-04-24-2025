import json
import pandas as pd
import streamlit as st
import snowflake.connector
import os
import boto3
import cv2
import easyocr
from langchain.chat_models import ChatOpenAI
from langchain.embeddings import OpenAIEmbeddings
from PIL import Image
from transformers import pipeline
from langchain.schema import HumanMessage, SystemMessage
from PyPDF2 import PdfReader
from langchain_experimental.agents import create_pandas_dataframe_agent
from fuzzywuzzy import process
from langchain.schema import AIMessage
from langchain_community.chat_models import BedrockChat
from langchain.chains import RetrievalQA
from langchain.chains.question_answering import load_qa_chain
from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
import os
import tempfile
def extract_text_from_pdf(pdf_path):
    """Extract text from a PDF file using PyPDF2."""
    try:
        reader = PdfReader(pdf_path)
        text = "\n".join([page.extract_text() for page in reader.pages if page.extract_text()])
        return text
    except Exception as e:
        return f"Error reading PDF: {str(e)}"
    
st.set_page_config(layout="wide")
if "messages" not in st.session_state:
    st.session_state.messages = []
if "summary" not in st.session_state:
    st.session_state.summary = None
st.markdown(
    """
    <style>
        [data-testid="stSidebar"] {
            background-color: #99F6F6;  /* Dark background */
            color: black;  /* White text */
        }
        [data-testid="stSidebar"] h1, 
        [data-testid="stSidebar"] h2, 
        [data-testid="stSidebar"] h3, 
        [data-testid="stSidebar"] p {
            color: black;  /* Ensures all text is white */
        }
    </style>
    """,
    unsafe_allow_html=True
)
with st.sidebar:
    API = st.text_input("Upload the API Key", type="password")
    Secure_Key = st.text_input("Upload the Secure Key",type="password")
    SNOWFLAKE_USER_input = st.text_input("Snowflake User", type="password")
    SNOWFLAKE_PASSWORD_input = st.text_input("Snowflake Password", type="password")

    if API and Secure_Key:
        st.session_state.API = API
        st.session_state.Secure_Key = Secure_Key

        bedrock_runtime = boto3.client(
            service_name="bedrock-runtime",
            region_name="us-east-1",
            aws_access_key_id=st.session_state.API,
            aws_secret_access_key=st.session_state.Secure_Key,
        )

        llm = BedrockChat(
            model_id="anthropic.claude-3-sonnet-20240229-v1:0",
            client=bedrock_runtime,
            model_kwargs={
                "max_tokens": 1000,
                "temperature": 0.1,
                "top_p": 0.9,
                "top_k": 50
            }
        )
    
        if SNOWFLAKE_USER_input and SNOWFLAKE_PASSWORD_input:
        # Snowflake connection details
            SNOWFLAKE_USER = SNOWFLAKE_USER_input
            SNOWFLAKE_PASSWORD = SNOWFLAKE_PASSWORD_input
            SNOWFLAKE_ACCOUNT = "c2gpartners.us-east-1"
            SNOWFLAKE_DATABASE = "DSX_DASHBOARDS"
            SNOWFLAKE_SCHEMA = "HUBSPOT_REPORTING"
            SNOWFLAKE_WAREHOUSE = "POWERHOUSE"

        # Establish connection to Snowflake
            conn = snowflake.connector.connect(
                user=SNOWFLAKE_USER,
                password=SNOWFLAKE_PASSWORD,
                account=SNOWFLAKE_ACCOUNT,
                warehouse=SNOWFLAKE_WAREHOUSE,
            )

            # SQL query to execute
            query = """
            SELECT DISTINCT_CAPABILITY AS "Number of capability",
                SNAPSHOT_DATE AS "Date of data pulled",
                PORTFOLIO_LEAD AS "Team",
                BD_LEAD AS "Deal Owner",
                PARTNER_SOURCE_TYPE AS "Partner Source Type",
                OPPORTUNITY_CREATE_DATE AS "Date",
                EXTRACT(MONTH FROM OPPORTUNITY_CREATE_DATE) AS "Opportunity Month",
                EXTRACT(YEAR FROM OPPORTUNITY_CREATE_DATE) AS "Opportunity Year",
                OPPORTUNITY_ID,
                OPPORTUNITY_NAME AS "Deal Name",
                EXPECTED_PROJECT_DURATION_IN_MONTHS AS "Expected Project duration",
                REVENUE_TYPE_ID,
                PIPELINE,
                DEAL_TYPE_ID,
                CASE 
                    WHEN OPPORTUNITY_STAGE_ID_DESC = 'STAGE 0' THEN '0-NEW'
                    WHEN OPPORTUNITY_STAGE_ID_DESC = 'STAGE 1' THEN '1-Connected to Meeting'
                    WHEN OPPORTUNITY_STAGE_ID_DESC = 'STAGE 2' THEN '2-Needs Expressed'
                    WHEN OPPORTUNITY_STAGE_ID_DESC = 'STAGE 3' THEN '3-Qualified Opportunity'
                    WHEN OPPORTUNITY_STAGE_ID_DESC = 'STAGE 4' THEN '4-Proposal Presented'
                    WHEN OPPORTUNITY_STAGE_ID_DESC = 'STAGE 5' THEN '5-Verbal Agreement'
                    WHEN OPPORTUNITY_STAGE_ID_DESC = 'STAGE 6' THEN '6-Contracted'
                    ELSE OPPORTUNITY_STAGE_ID_DESC
                END AS "Stage type",
                AMOUNT,
                TCV,
                MRR,
                ICV
            FROM DSX_DASHBOARDS_SANDBOX.HUBSPOT_REPORTING.VW_DEALS_LINE_ITEMS_DATA
            WHERE SNAPSHOT_DATETIME = 
                    (
                        SELECT MAX(SNAPSHOT_DATETIME)
                        FROM DSX_DASHBOARDS_SANDBOX.HUBSPOT_REPORTING.VW_DEALS_LINE_ITEMS_DATA
                    )
            """
            #pdf_path = r"C:\Users\LokeshRamesh\Documents\co_10 training\AI\Linkedin Chatbot\Power_BI_User_Guide_1.pdf"
            #user_guide_text = extract_text_from_pdf(pdf_path)

            # Split text into chunks for better retrieval
            #text_splitter = RecursiveCharacterTextSplitter(
            #    chunk_size=500, chunk_overlap=50, length_function=len
            #)
            #text_chunks = text_splitter.split_text(user_guide_text)

            # Create text embeddings
            #embedding_model = OpenAIEmbeddings(model="text-embedding-ada-002")
            #vector_store = FAISS.from_texts(texts=text_chunks, embedding=embedding_model)
            #retriever = vector_store.as_retriever()


            Snowflack_data = pd.read_sql(query, conn)

            json_path = r"DataModelSchema.json"
            excel_path = r"Data DictionaryChat bot.xlsx"

            json_data = pd.read_json(json_path, encoding='utf-16')
            df = pd.DataFrame()

            # Process JSON data
            table_1 = list(json_data["model"]['tables'])
            for i in range(len(table_1)):
                table = table_1[i]
                if 'measures' in table:
                    df = pd.concat([df, pd.DataFrame(table['measures'])], ignore_index=True)
            Measure_Table = df[["name", "expression"]]
            Measure_Table = Measure_Table.rename(columns={"expression": "DAX", "name": "Dax Name"})

            df_1 = pd.DataFrame(columns=['Table Name', 'Column Name'])
            tables = json_data["model"]['tables']
            for table in tables:
                if 'columns' in table:
                    for column in table['columns']:
                        df_1 = pd.concat([df_1, pd.DataFrame({'Table Name': [table['name']], 'Column Name': [column['name']], 'Data Type': [column['dataType']]})], ignore_index=True)

            # Process Excel data
            xls_data = pd.read_excel(excel_path)



            #PDF = RetrievalQA.from_chain_type(llm=llm,chain_type="stuff",retriever=retriever)
            DAX = create_pandas_dataframe_agent(llm=llm,df = Measure_Table,allow_dangerous_code=True, handle_parsing_errors=True,verbose=True,number_of_head_rows= Measure_Table.shape[0])
            Table = create_pandas_dataframe_agent(llm=llm,df = df_1,allow_dangerous_code=True, handle_parsing_errors=True,verbose=True,number_of_head_rows= df_1.shape[0])
            Excel = create_pandas_dataframe_agent(llm=llm,df = xls_data,allow_dangerous_code=True, handle_parsing_errors=True,verbose=True,number_of_head_rows= xls_data.shape[0])
            Snowflack = create_pandas_dataframe_agent(llm=llm,df = Snowflack_data,allow_dangerous_code=True, handle_parsing_errors=True,verbose=True)

            # Initialize session state for chat history

            st.sidebar.title("ðŸ’¡ How to Chat with the Bot")
            st.sidebar.write("""
            âœ… Use **keywords** in your query to select the right data source:
            - **DAX**: Questions about calculations, formulas, or Power BI measures.
            - **TABLE**: Structure of Power BI tables.
            - **DATA**: Queries related to numbers and stored data.
            - **DICTIONARY**: Definitions of Power BI terms.
            - **GUIDE**: User Manual or User Guide.

            ðŸ“Œ **Example Queries:**
            - `"Give me the % of deals based on each stage from the data"` â†’ Uses **Data**
            - `"Give me the calculation for TCV"` â†’ Uses **Dax**
            - `"What does TCV means ?"` â†’ Uses **Dictionary**
            - `"Give the table names present"` â†’ Uses **Table**
            - `"Analyze the data and identify all possible reasons why the TCV in February 2024 is higher compared to other months. Consider factors such as team performance, deal size, client activity, or any noticeable trends or anomalies."` â†’ Uses **Data**
            """)


            st.title("Power BI Smart Bot")

            # Chat response container
            response_container = st.container()
            with st.container(border=True):
                response_container = st.container(height=450)
                


            # Define mapping of topics to their corresponding AI agents
                agent_mapping = {
                    "DAX": DAX,
                    "CALCULATION": DAX,
                    "MEASURE": DAX,
                    "TABLE": Table,
                    "POWER BI TABLE": Table,
                    "TABLE IN POWER BI": Table,
                    "DATA": Snowflack,  # Fixed typo
                    "NUMBER": Snowflack,
                    "DICTIONARY": Excel,
                    "MEANING": Excel,
                    "MEANS": Excel,
                    "DEFINITION": Excel
                }

                # Alternative keywords for better recognition
                alternative_keywords = {
                    "DAX": ["dax query", "dax formula", "power bi dax", "calculate measure"],
                    "CALCULATION": ["formula", "compute", "aggregation", "math", "sum"],
                    "MEASURE": ["metric", "KPI", "indicator"],
                    "TABLE": ["dataset", "record", "row", "column"],
                    "POWER BI TABLE": ["data model", "bi table", "table structure"],
                    "DATA": ["info", "database", "dataset"],
                    "NUMBER": ["integer", "float", "value"],
                    "DICTIONARY": ["lexicon", "glossary"],
                    "MEANING": ["explanation", "clarification"],
                    "DEFINITION": ["describe", "what is", "explain"]
                }

                # Function to determine the best matching agent
                def find_best_match(user_input):
                    user_input = user_input.upper()

                    # Check for direct keyword matches first
                    for keyword, agent in agent_mapping.items():
                        if keyword in user_input:
                            return agent

                    # Use fuzzy matching for better recognition
                    all_keywords = list(agent_mapping.keys()) + [kw.upper() for words in alternative_keywords.values() for kw in words]
                    match = process.extractOne(user_input, all_keywords)

                    if match and match[1] > 70:  # Confidence threshold
                        best_match = match[0]
                        for keyword, synonyms in alternative_keywords.items():
                            if best_match in [s.upper() for s in synonyms] or best_match == keyword:
                                return agent_mapping[keyword]

                    return None  # No strong match found
                if st.button('Clear Chat'):
                    st.session_state.messages = []

                # User input prompt
                if prompt := st.chat_input("Ask your question here"):
                    st.session_state.messages.append({"role": "user", "content": prompt})

                    # Build conversation history safely
                    conversation_history = "\n".join([
                        msg.get("content", "") for msg in st.session_state.messages if isinstance(msg, dict)
                    ])

                    try:
                        selected_agent = find_best_match(prompt)

                        if selected_agent:
                            response_content = selected_agent.invoke(conversation_history)
                        elif "GUIDE" in prompt.upper() or "PDF" in prompt.upper():
                            response_content = PDF.invoke(conversation_history).get("result"," ")
                        else:
                            response_content = llm.invoke(conversation_history)
                            if isinstance(response_content, AIMessage):
                                response_content = response_content.content  # Directly access the content attribute

                                # Ensure it's a string
                                response_content = str(response_content)

                                if not response_content.strip():
                                    response_content = "No valid response received."
                            
                        response_content = (
                                response_content.get("output", "") 
                                if isinstance(response_content, dict) else str(response_content))
                        if not response_content.strip():
                                    response_content = "No valid response received."
                    except Exception as e:
                        response_content = f"Error occurred: {str(e)}"

                    # Store AI response
                    st.session_state.messages.append({"role": "assistant", "content": response_content})
                

                # Display chat messages
                with response_container:
                    for message in st.session_state.messages:
                        with st.chat_message(message["role"]):
                            st.markdown(f'<div style="font-size: small;">{message["content"]}</div>', unsafe_allow_html=True)

with st.container(border=True):
    power_bi_url = "https://app.powerbi.com/reportEmbed?reportId=2e6587b4-0312-4716-b90f-9e6e09928448&autoAuth=true&ctid=b1aae949-a5ef-4815-b7af-f7c4aa546b28"    # Custom CSS for Full-Screen Power BI Embed
    st.markdown(
        """
        <style>
        /* Remove default Streamlit padding */
        [data-testid="stAppViewContainer"] {
            padding: 0 !important;
        }

        /* Make the iframe cover full width */
        iframe {
            height: 95vh !important; /* Covers 95% of the viewport height */
            width: 100% !important; /* Covers full width */
        }
        </style>
        """,
        unsafe_allow_html=True,
    )

        # Embed Power BI Report using HTML
    st.markdown(
        f"""
        <iframe title="Power BI Report" width="100%" height="95vh" 
        src="{power_bi_url}" frameborder="0" allowFullScreen="true"></iframe>
        """,
        unsafe_allow_html=True,
    )

st.title("Image analyzer")
st.write("Upload an image of a dashboard to analyze its contents.")
image_path = st.file_uploader("Upload an image", type=["jpg", "jpeg", "png"], key="image_uploader")

if image_path:
    image = Image.open(image_path)
    st.image(image, caption="ðŸ“¸ Uploaded Dashboard Image", use_column_width=True)

    with tempfile.NamedTemporaryFile(delete=False, suffix=".png") as tmp_file:
        image.save(tmp_file.name)
        image_path = tmp_file.name
        reader = easyocr.Reader(['en'])
        img = cv2.imread(image_path)
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)


        ocr_results = reader.readtext(gray)
        extracted_text = [text[1] for text in ocr_results]
        extracted_numbers = [word for text in extracted_text for word in text.split() if word.replace(",", "").replace(".", "").isdigit()]

        # Detect active filter (Assuming a placeholder function detect_active_filter)
        #active_filter = detect_active_filter(image_path)

        description = (f"Dashboard contains charts/tables. "
                        f"Detected text: {'; '.join(extracted_text)}. "
                        f"Numbers: {', '.join(extracted_numbers)}. ")
                        #f"Active filter: {active_filter}.")
        
        system_message = SystemMessage(content="You are a skilled Image analyst.")

# Define the user's prompt
        user_message = HumanMessage(content=
        f"""You're analyzing a dashboard based on the visual input. Summarize the dashboard content in a detailed yet user-friendly way, using only the information visible in the dashboard. Avoid technical or backend detailsâ€”assume the user can only see what's presented on the screen.
Here         is the extracted text from the dashboard: {description}""")

        st.write("Analyzing the image...")
        response = llm.invoke([system_message, user_message])
        st.write(response.content)
